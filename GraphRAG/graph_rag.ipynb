{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables from a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: sk-proj-8JG....\n",
      "OPENAI_API_MODEL: gpt-4o-2024-08-06\n",
      "OPENAI_API_EMBEDDING_MODEL: text-embedding-3-large\n",
      "FAISS_INDEX_NAME: YOUR_FAISS_INDEX_NAME\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"OPENAI_API_KEY: {OPENAI_API_KEY}\")\n",
    "\n",
    "OPENAI_API_MODEL = os.getenv(\"OPENAI_API_MODEL\")\n",
    "print(f\"OPENAI_API_MODEL: {OPENAI_API_MODEL}\")\n",
    "\n",
    "OPENAI_API_EMBEDDING_MODEL = os.getenv(\"OPENAI_API_EMBEDDING_MODEL\")\n",
    "print(f\"OPENAI_API_EMBEDDING_MODEL: {OPENAI_API_EMBEDDING_MODEL}\")\n",
    "\n",
    "FAISS_INDEX_NAME = os.getenv(\"FAISS_INDEX_NAME\")\n",
    "print(f\"FAISS_INDEX_NAME: {FAISS_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply async mechanism\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/fymbnzx50nn5sgwvw84wr0040000gn/T/ipykernel_90411/2408750481.py:40: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from helper_functions import *\n",
      "/Users/mac/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.3.6, however version 1.4.2 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))) \u001b[38;5;66;03m# Add the parent directory to the path sicnce we work with notebooks\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_rag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Load environment variables from a .env file\u001b[39;00m\n\u001b[1;32m     44\u001b[0m load_dotenv()\n",
      "File \u001b[0;32m/Volumes/Data/OpenSources/SOFAR_DATAS/RAG/RAG-PlayGround/GraphRAG/evaluation/evaluate_rag.py:62\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     50\u001b[0m         LLMTestCase(\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mquestion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Define evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m correctness_metric \u001b[38;5;241m=\u001b[39m \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCorrectness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXPECTED_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACTUAL_OUTPUT\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDetermine whether the actual output is factually correct based on the expected output.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m faithfulness_metric \u001b[38;5;241m=\u001b[39m FaithfulnessMetric(\n\u001b[1;32m     75\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     76\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     include_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     80\u001b[0m relevance_metric \u001b[38;5;241m=\u001b[39m ContextualRelevancyMetric(\n\u001b[1;32m     81\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     include_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     84\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/metrics/g_eval/g_eval.py:90\u001b[0m, in \u001b[0;36mGEval.__init__\u001b[0;34m(self, name, evaluation_params, criteria, evaluation_steps, model, threshold, async_mode, strict_mode, verbose_mode, _include_g_eval_suffix)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must not be an empty list. Either omit evaluation steps or include a non-empty list of steps.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriteria \u001b[38;5;241m=\u001b[39m criteria\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model_name()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_steps \u001b[38;5;241m=\u001b[39m evaluation_steps\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/metrics/utils.py:266\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Otherwise (the model is a string or None), we initialize a GPTModel and use as a native model\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/models/gpt_model.py:103\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[0;34m(self, model, _openai_api_key, base_url, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/models/base_model.py:35\u001b[0m, in \u001b[0;36mDeepEvalBaseLLM.__init__\u001b[0;34m(self, model_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/deepeval/models/gpt_model.py:156\u001b[0m, in \u001b[0;36mGPTModel.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomChatOpenAI(\n\u001b[1;32m    145\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m    146\u001b[0m         openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_openai_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/langchain_core/load/serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    550\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import Relevant Libraries\n",
    "\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "import heapq\n",
    "import tiktoken\n",
    "import asyncio\n",
    "from aiolimiter import AsyncLimiter\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from spacy.cli import download\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "nltk.download('punkt', quiet=True) # download the punkt tokenizer for sentence tokenization\n",
    "nltk.download('wordnet', quiet=True) # download the wordnet corpus for lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the documents from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory not found: 'data2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the documents with the type of files: .txt, .md, .pdf, .csv, .json\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata2\u001b[39m\u001b[38;5;124m\"\u001b[39m, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_multithreading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Change the directory path to the directory containing your documents\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments [0]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocuments[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/sofardatasai-rag-engine/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:123\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected directory, got file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory not found: 'data2'"
     ]
    }
   ],
   "source": [
    "# Load the documents with the type of files: .txt, .md, .pdf, .csv, .json\n",
    "loader = DirectoryLoader(\"data2\", show_progress=True, use_multithreading=True) # Change the directory path to the directory containing your documents\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"documents length: {len(documents)}\")\n",
    "print(f\"documents [0]: {documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DocumentProcessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DocumentProcessor class\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor with a text splitter and OpenAI embeddings.\n",
    "\n",
    "        Attributes:\n",
    "        - text_splitter: An instance of RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "        - embeddings: An instance of OpenAIEmbeddings used for embedding documents.\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        self.embeddings = OpenAIEmbeddings(model=OPENAI_API_EMBEDDING_MODEL)\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into smaller chunks and creating a vector store.\n",
    "\n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - splits (list of str): The list of split document chunks.\n",
    "          - vector_store (FAISS): A FAISS vector store created from the split document chunks and their embeddings.\n",
    "        \"\"\"\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "\n",
    "        # Check if the index already exists\n",
    "        if os.path.exists(FAISS_INDEX_NAME):\n",
    "            # Load the existing FAISS index\n",
    "            vector_store = FAISS.load_local(FAISS_INDEX_NAME, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            # Create a FAISS index from the text documents\n",
    "            vector_store = FAISS.from_documents(splits, self.embeddings)\n",
    "\n",
    "            # Save the FAISS index to disk\n",
    "            FAISS.save_local(vector_store, FAISS_INDEX_NAME)\n",
    "        return splits, vector_store\n",
    "\n",
    "    def create_embeddings_batch(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Creates embeddings for a list of texts in batches.\n",
    "\n",
    "        Args:\n",
    "        - texts (list of str): A list of texts to be embedded.\n",
    "        - batch_size (int, optional): The number of texts to process in each batch. Default is 32.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embeddings.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_similarity_matrix(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes a cosine similarity matrix for a given set of embeddings.\n",
    "\n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the input embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Concepts class\n",
    "class Concepts(BaseModel):\n",
    "    concepts_list: List[str] = Field(description=\"List of concepts\")\n",
    "\n",
    "# Define the KnowledgeGraph class\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self, tpm):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeGraph with a graph, lemmatizer, and NLP model.\n",
    "\n",
    "        Attributes:\n",
    "        - graph: An instance of a networkx Graph.\n",
    "        - lemmatizer: An instance of WordNetLemmatizer.\n",
    "        - concept_cache: A dictionary to cache extracted concepts.\n",
    "        - nlp: An instance of a spaCy NLP model.\n",
    "        - edges_threshold: A float value that sets the threshold for adding edges based on similarity.\n",
    "        - tpm: Aka token per minute, we need to limit the number of tokens to avoid high cost and exceed the TPM limit\n",
    "        \"\"\"\n",
    "        self.graph = nx.Graph()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.concept_cache = {}\n",
    "        self.nlp = self._load_spacy_model()\n",
    "        self.edges_threshold = 0.8\n",
    "        self.tpm = tpm\n",
    "\n",
    "    async def build_graph(self, splits, llm, embedding_model):\n",
    "        \"\"\"\n",
    "        Builds the knowledge graph by adding nodes, creating embeddings, extracting concepts, and adding edges.\n",
    "\n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self._add_nodes(splits)\n",
    "        embeddings = await self._create_embeddings(splits, embedding_model)\n",
    "        print(f\"embeddings length: {len(embeddings)}\")\n",
    "        await self._extract_concepts(splits, llm)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_nodes(self, splits):\n",
    "        \"\"\"\n",
    "        Adds nodes to the graph from the document splits.\n",
    "\n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        for i, split in enumerate(splits):\n",
    "            self.graph.add_node(i, content=split.page_content)\n",
    "\n",
    "    async def _create_embeddings(self, splits, embedding_model):\n",
    "        \"\"\"\n",
    "        Creates embeddings for the document splits using the embedding model with batch processing and rate limiting.\n",
    "\n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the document splits.\n",
    "        \"\"\"\n",
    "        texts = [split.page_content for split in splits]\n",
    "\n",
    "        # Initialize tokenizer for counting tokens\n",
    "        tokenizer = tiktoken.encoding_for_model(OPENAI_API_MODEL)\n",
    "\n",
    "        async def process_batch(batch):\n",
    "            max_retries = 5\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return embedding_model.embed_documents(batch)\n",
    "                except Exception as e:\n",
    "                    if \"rate_limit_exceeded\" in str(e) and attempt < max_retries - 1:\n",
    "                        wait_time = 2 ** attempt  # Exponential backoff\n",
    "                        print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        all_embeddings = []\n",
    "        batch = []\n",
    "        batch_tokens = 0\n",
    "        max_batch_tokens = min(self.tpm, 8000)  # Limit batch size to 8000 tokens or tpm, whichever is smaller\n",
    "\n",
    "        for text in texts:\n",
    "            text_tokens = len(tokenizer.encode(text))\n",
    "            if batch_tokens + text_tokens > max_batch_tokens:\n",
    "                # Process current batch\n",
    "                batch_embeddings = await process_batch(batch)\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "                # Reset batch and wait for rate limit\n",
    "                batch = []\n",
    "                batch_tokens = 0\n",
    "                await asyncio.sleep(60)  # Wait for 60 seconds before next batch\n",
    "\n",
    "            batch.append(text)\n",
    "            batch_tokens += text_tokens\n",
    "\n",
    "        # Process any remaining texts in the last batch\n",
    "        if batch:\n",
    "            batch_embeddings = await process_batch(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return np.array(all_embeddings)\n",
    "\n",
    "    def _compute_similarities(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity matrix for the embeddings.\n",
    "\n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)\n",
    "\n",
    "    def _load_spacy_model(self):\n",
    "        \"\"\"\n",
    "        Loads the spaCy NLP model, downloading it if necessary.\n",
    "\n",
    "        Args:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - spacy.Language: An instance of a spaCy NLP model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy model...\")\n",
    "            download(\"en_core_web_sm\")\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "    async def _extract_concepts_and_entities(self, content, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts and named entities from the content using spaCy and a large language model.\n",
    "\n",
    "        Args:\n",
    "        - content (str): The content from which to extract concepts and entities.\n",
    "        - llm: An instance of a large language model.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of extracted concepts and entities.\n",
    "        \"\"\"\n",
    "        if content in self.concept_cache:\n",
    "            return self.concept_cache[content]\n",
    "\n",
    "        # Extract named entities using spaCy\n",
    "        doc = self.nlp(content)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
    "\n",
    "        # Extract general concepts using LLM\n",
    "        concept_extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
    "        )\n",
    "        concept_chain = concept_extraction_prompt | llm.with_structured_output(Concepts)\n",
    "        general_concepts = (await concept_chain.ainvoke({\"text\": content})).concepts_list\n",
    "\n",
    "        # Combine named entities and general concepts\n",
    "        all_concepts = list(set(named_entities + general_concepts))\n",
    "\n",
    "        self.concept_cache[content] = all_concepts\n",
    "        return all_concepts\n",
    "\n",
    "    async def _extract_concepts(self, splits, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts for all document splits using asynchronous processing and rate limiting.\n",
    "\n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        # Calculate tokens per second based on tokens per minute\n",
    "        rate_limiter = AsyncLimiter(self.tpm, 60)\n",
    "\n",
    "        async def process_split(split, index):\n",
    "            async with rate_limiter:\n",
    "                concepts = await self._extract_concepts_and_entities(split.page_content, llm)\n",
    "                self.graph.nodes[index]['concepts'] = concepts\n",
    "\n",
    "        batch_size = 10  # Adjust this value based on your needs\n",
    "        for i in range(0, len(splits), batch_size):\n",
    "            batch = splits[i:i+batch_size]\n",
    "            tasks = [process_split(split, i+j) for j, split in enumerate(batch)]\n",
    "            await tqdm_asyncio.gather(*tasks, desc=f\"Extracting concepts (batch {i//batch_size + 1})\")\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        \"\"\"\n",
    "        Adds edges to the graph based on the similarity of embeddings and shared concepts.\n",
    "\n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings for the document splits.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        similarity_matrix = self._compute_similarities(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "\n",
    "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
    "            for node2 in range(node1 + 1, num_nodes):\n",
    "                similarity_score = similarity_matrix[node1][node2]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(self.graph.nodes[node2]['concepts'])\n",
    "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
    "                    self.graph.add_edge(node1, node2, weight=edge_weight,\n",
    "                                        similarity=similarity_score,\n",
    "                                        shared_concepts=list(shared_concepts))\n",
    "\n",
    "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
    "        \"\"\"\n",
    "        Calculates the weight of an edge based on similarity score and shared concepts.\n",
    "\n",
    "        Args:\n",
    "        - node1 (int): The first node.\n",
    "        - node2 (int): The second node.\n",
    "        - similarity_score (float): The similarity score between the nodes.\n",
    "        - shared_concepts (set): The set of shared concepts between the nodes.\n",
    "        - alpha (float, optional): The weight of the similarity score. Default is 0.7.\n",
    "        - beta (float, optional): The weight of the shared concepts. Default is 0.3.\n",
    "\n",
    "        Returns:\n",
    "        - float: The calculated weight of the edge.\n",
    "        \"\"\"\n",
    "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
    "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
    "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
    "\n",
    "    def _lemmatize_concept(self, concept):\n",
    "        \"\"\"\n",
    "        Lemmatizes a given concept.\n",
    "\n",
    "        Args:\n",
    "        - concept (str): The concept to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        - str: The lemmatized concept.\n",
    "        \"\"\"\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AnswerCheck class\n",
    "class AnswerCheck(BaseModel):\n",
    "    is_complete: bool = Field(description=\"Whether the current context provides a complete answer to the query.\")\n",
    "    answer: str = Field(description=\"The current answer based on the context, if any.\")\n",
    "\n",
    "# Define the QueryEngine class\n",
    "class QueryEngine:\n",
    "    def __init__(self, vector_store: VectorStore, knowledge_graph: KnowledgeGraph, llm: ChatOpenAI):\n",
    "        self.vector_store = vector_store\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "        self.llm = llm\n",
    "        self.max_context_length = 64000\n",
    "        self.answer_check_chain = self._create_answer_check_chain()\n",
    "\n",
    "    def _create_answer_check_chain(self):\n",
    "        \"\"\"\n",
    "        Creates a chain to check if the context provides a complete answer to the query.\n",
    "\n",
    "        Args:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - Chain: A chain to check if the context provides a complete answer.\n",
    "        \"\"\"\n",
    "        answer_check_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the query: '{query}'\\n\\nAnd the current context:\\n{context}\\n\\nDoes this context provide a complete answer to the query? If yes, provide the answer. If no, state that the answer is incomplete.\\n\\nIs complete answer (Yes/No):\\nAnswer (if complete):\"\n",
    "        )\n",
    "        return answer_check_prompt | self.llm.with_structured_output(AnswerCheck)\n",
    "\n",
    "    def _check_answer(self, query: str, context: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Checks if the current context provides a complete answer to the query.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        - context (str): The current context.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - is_complete (bool): Whether the context provides a complete answer.\n",
    "          - answer (str): The answer based on the context, if complete.\n",
    "        \"\"\"\n",
    "        response = self.answer_check_chain.invoke({\"query\": query, \"context\": context})\n",
    "        return response.is_complete, response.answer\n",
    "\n",
    "\n",
    "\n",
    "    def _expand_context(self, query: str, relevant_docs) -> Tuple[str, List[int], Dict[int, str], str]:\n",
    "        \"\"\"\n",
    "        Expands the context by traversing the knowledge graph using a Dijkstra-like approach.\n",
    "\n",
    "        This method implements a modified version of Dijkstra's algorithm to explore the knowledge graph,\n",
    "        prioritizing the most relevant and strongly connected information. The algorithm works as follows:\n",
    "\n",
    "        1. Initialize:\n",
    "           - Start with nodes corresponding to the most relevant documents.\n",
    "           - Use a priority queue to manage the traversal order, where priority is based on connection strength.\n",
    "           - Maintain a dictionary of best known \"distances\" (inverse of connection strengths) to each node.\n",
    "\n",
    "        2. Traverse:\n",
    "           - Always explore the node with the highest priority (strongest connection) next.\n",
    "           - For each node, check if we've found a complete answer.\n",
    "           - Explore the node's neighbors, updating their priorities if a stronger connection is found.\n",
    "\n",
    "        3. Concept Handling:\n",
    "           - Track visited concepts to guide the exploration towards new, relevant information.\n",
    "           - Expand to neighbors only if they introduce new concepts.\n",
    "\n",
    "        4. Termination:\n",
    "           - Stop if a complete answer is found.\n",
    "           - Continue until the priority queue is empty (all reachable nodes explored).\n",
    "\n",
    "        This approach ensures that:\n",
    "        - We prioritize the most relevant and strongly connected information.\n",
    "        - We explore new concepts systematically.\n",
    "        - We find the most relevant answer by following the strongest connections in the knowledge graph.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        - relevant_docs (List[Document]): A list of relevant documents to start the traversal.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - expanded_context (str): The accumulated context from traversed nodes.\n",
    "          - traversal_path (List[int]): The sequence of node indices visited.\n",
    "          - filtered_content (Dict[int, str]): A mapping of node indices to their content.\n",
    "          - final_answer (str): The final answer found, if any.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        expanded_context = \"\"\n",
    "        traversal_path = []\n",
    "        visited_concepts = set()\n",
    "        filtered_content = {}\n",
    "        final_answer = \"\"\n",
    "\n",
    "        priority_queue = []\n",
    "        distances = {}  # Stores the best known \"distance\" (inverse of connection strength) to each node\n",
    "\n",
    "        print(\"\\nTraversing the knowledge graph:\")\n",
    "\n",
    "        # Initialize priority queue with closest nodes from relevant docs\n",
    "        for doc in relevant_docs:\n",
    "            # Find the most similar node in the knowledge graph for each relevant document\n",
    "            closest_nodes = self.vector_store.similarity_search_with_score(doc.page_content, k=1)\n",
    "            closest_node_content, similarity_score = closest_nodes[0]\n",
    "\n",
    "            # Get the corresponding node in our knowledge graph\n",
    "            closest_node = next(n for n in self.knowledge_graph.graph.nodes if self.knowledge_graph.graph.nodes[n]['content'] == closest_node_content.page_content)\n",
    "\n",
    "            # Initialize priority (inverse of similarity score for min-heap behavior)\n",
    "            priority = 1 / similarity_score\n",
    "            heapq.heappush(priority_queue, (priority, closest_node))\n",
    "            distances[closest_node] = priority\n",
    "\n",
    "        step = 0\n",
    "        while priority_queue:\n",
    "            # Get the node with the highest priority (lowest distance value)\n",
    "            current_priority, current_node = heapq.heappop(priority_queue)\n",
    "\n",
    "            # Skip if we've already found a better path to this node\n",
    "            if current_priority > distances.get(current_node, float('inf')):\n",
    "                continue\n",
    "\n",
    "            if current_node not in traversal_path:\n",
    "                step += 1\n",
    "                traversal_path.append(current_node)\n",
    "                node_content = self.knowledge_graph.graph.nodes[current_node]['content']\n",
    "                node_concepts = self.knowledge_graph.graph.nodes[current_node]['concepts']\n",
    "\n",
    "                # Add node content to our accumulated context\n",
    "                filtered_content[current_node] = node_content\n",
    "                expanded_context += \"\\n\" + node_content if expanded_context else node_content\n",
    "\n",
    "                # Log the current step for debugging and visualization\n",
    "                print(f\"\\nStep {step} - Node {current_node}:\")\n",
    "                print(f\"Content: {node_content[:100]}...\")\n",
    "                print(f\"Concepts: {', '.join(node_concepts)}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                # Check if we have a complete answer with the current context\n",
    "                is_complete, answer = self._check_answer(query, expanded_context)\n",
    "                if is_complete:\n",
    "                    final_answer = answer\n",
    "                    break\n",
    "\n",
    "                # Process the concepts of the current node\n",
    "                node_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in node_concepts)\n",
    "                if not node_concepts_set.issubset(visited_concepts):\n",
    "                    visited_concepts.update(node_concepts_set)\n",
    "\n",
    "                    # Explore neighbors\n",
    "                    for neighbor in self.knowledge_graph.graph.neighbors(current_node):\n",
    "                        edge_data = self.knowledge_graph.graph[current_node][neighbor]\n",
    "                        edge_weight = edge_data['weight']\n",
    "\n",
    "                        # Calculate new distance (priority) to the neighbor\n",
    "                        # Note: We use 1 / edge_weight because higher weights mean stronger connections\n",
    "                        distance = current_priority + (1 / edge_weight)\n",
    "\n",
    "                        # If we've found a stronger connection to the neighbor, update its distance\n",
    "                        if distance < distances.get(neighbor, float('inf')):\n",
    "                            distances[neighbor] = distance\n",
    "                            heapq.heappush(priority_queue, (distance, neighbor))\n",
    "\n",
    "                            # Process the neighbor node if it's not already in our traversal path\n",
    "                            if neighbor not in traversal_path:\n",
    "                                step += 1\n",
    "                                traversal_path.append(neighbor)\n",
    "                                neighbor_content = self.knowledge_graph.graph.nodes[neighbor]['content']\n",
    "                                neighbor_concepts = self.knowledge_graph.graph.nodes[neighbor]['concepts']\n",
    "\n",
    "                                filtered_content[neighbor] = neighbor_content\n",
    "                                expanded_context += \"\\n\" + neighbor_content if expanded_context else neighbor_content\n",
    "\n",
    "                                # Log the neighbor node information\n",
    "                                print(f\"\\nStep {step} - Node {neighbor} (neighbor of {current_node}):\")\n",
    "                                print(f\"Content: {neighbor_content[:100]}...\")\n",
    "                                print(f\"Concepts: {', '.join(neighbor_concepts)}\")\n",
    "                                print(\"-\" * 50)\n",
    "\n",
    "                                # Check if we have a complete answer after adding the neighbor's content\n",
    "                                is_complete, answer = self._check_answer(query, expanded_context)\n",
    "                                if is_complete:\n",
    "                                    final_answer = answer\n",
    "                                    break\n",
    "\n",
    "                                # Process the neighbor's concepts\n",
    "                                neighbor_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in neighbor_concepts)\n",
    "                                if not neighbor_concepts_set.issubset(visited_concepts):\n",
    "                                    visited_concepts.update(neighbor_concepts_set)\n",
    "\n",
    "                # If we found a final answer, break out of the main loop\n",
    "                if final_answer:\n",
    "                    break\n",
    "\n",
    "        # If we haven't found a complete answer, generate one using the LLM\n",
    "        if not final_answer:\n",
    "            print(\"\\nGenerating final answer...\")\n",
    "            response_prompt = PromptTemplate(\n",
    "                input_variables=[\"query\", \"context\"],\n",
    "                template=\"<Role>You are a helpful assistant, providing comprehensive answers to the questions based on the relevant context.</Role> \\\n",
    "                        <Context>{context}</Context> \\\n",
    "                        <Question>{query}</Question> \\\n",
    "                        <Answer>Answer:</Answer> \\\n",
    "                    \"\n",
    "            )\n",
    "            response_chain = response_prompt | self.llm\n",
    "            input_data = {\"query\": query, \"context\": expanded_context}\n",
    "            final_answer = response_chain.invoke(input_data)\n",
    "\n",
    "        return expanded_context, traversal_path, filtered_content, final_answer\n",
    "\n",
    "    async def query(self, query: str) -> Tuple[str, List[int], Dict[int, str]]:\n",
    "        \"\"\"\n",
    "        Processes a query by retrieving relevant documents, expanding the context, and generating the final answer.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - final_answer (str): The final answer to the query.\n",
    "          - traversal_path (list): The traversal path of nodes in the knowledge graph.\n",
    "          - filtered_content (dict): The filtered content of nodes.\n",
    "        \"\"\"\n",
    "        with get_openai_callback() as cb:\n",
    "            print(f\"\\nProcessing query: {query}\")\n",
    "            relevant_docs = await self._retrieve_relevant_documents(query)\n",
    "            #  apply reranking for relevant_docs with cohere api\n",
    "            expanded_context, traversal_path, filtered_content, final_answer = self._expand_context(query, relevant_docs)\n",
    "\n",
    "            if not final_answer:\n",
    "                print(\"\\nGenerating final answer...\")\n",
    "                response_prompt = PromptTemplate(\n",
    "                    input_variables=[\"query\", \"context\"],\n",
    "                    template=\"<Role>You are a helpful assistant, providing comprehensive answers to the questions based on the relevant context.</Role> \\\n",
    "                        <Context>{context}</Context> \\\n",
    "                        <Question>{query}</Question> \\\n",
    "                        <Answer>Answer:</Answer> \\\n",
    "                    \"\n",
    "                )\n",
    "\n",
    "                response_chain = response_prompt | self.llm\n",
    "                input_data = {\"query\": query, \"context\": expanded_context}\n",
    "                response = response_chain.invoke(input_data)\n",
    "                final_answer = response\n",
    "            else:\n",
    "                print(\"\\nComplete answer found during traversal.\")\n",
    "\n",
    "            print(f\"\\nFinal Answer: {final_answer}\")\n",
    "            print(f\"\\nTotal Tokens: {cb.total_tokens}\")\n",
    "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "        return final_answer, traversal_path, filtered_content\n",
    "\n",
    "    async def _retrieve_relevant_documents(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieves relevant documents based on the query using the vector store.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of relevant documents.\n",
    "        \"\"\"\n",
    "        print(\"\\nRetrieving relevant documents...\")\n",
    "        retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        compressor = LLMChainExtractor.from_llm(self.llm)\n",
    "        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "        return compression_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define the Visualizer class\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def visualize_traversal(graph, traversal_path):\n",
    "        \"\"\"\n",
    "        Visualizes the traversal path on the knowledge graph with nodes, edges, and traversal path highlighted.\n",
    "\n",
    "        Args:\n",
    "        - graph (networkx.Graph): The knowledge graph containing nodes and edges.\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        traversal_graph = nx.DiGraph()\n",
    "\n",
    "        # Add nodes and edges from the original graph\n",
    "        for node in graph.nodes():\n",
    "            traversal_graph.add_node(node)\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            traversal_graph.add_edge(u, v, **data)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "        # Generate positions for all nodes\n",
    "        pos = nx.spring_layout(traversal_graph, k=1, iterations=50)\n",
    "\n",
    "        # Draw regular edges with color based on weight\n",
    "        edges = traversal_graph.edges()\n",
    "        edge_weights = [traversal_graph[u][v].get('weight', 0.5) for u, v in edges]\n",
    "        nx.draw_networkx_edges(traversal_graph, pos,\n",
    "                               edgelist=edges,\n",
    "                               edge_color=edge_weights,\n",
    "                               edge_cmap=plt.cm.Blues,\n",
    "                               width=2,\n",
    "                               ax=ax)\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               node_color='lightblue',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        # Draw traversal path with curved arrows\n",
    "        edge_offset = 0.1\n",
    "        for i in range(len(traversal_path) - 1):\n",
    "            start = traversal_path[i]\n",
    "            end = traversal_path[i + 1]\n",
    "            start_pos = pos[start]\n",
    "            end_pos = pos[end]\n",
    "\n",
    "            # Calculate control point for curve\n",
    "            mid_point = ((start_pos[0] + end_pos[0]) / 2, (start_pos[1] + end_pos[1]) / 2)\n",
    "            control_point = (mid_point[0] + edge_offset, mid_point[1] + edge_offset)\n",
    "\n",
    "            # Draw curved arrow\n",
    "            arrow = patches.FancyArrowPatch(start_pos, end_pos,\n",
    "                                            connectionstyle=f\"arc3,rad={0.3}\",\n",
    "                                            color='red',\n",
    "                                            arrowstyle=\"->\",\n",
    "                                            mutation_scale=20,\n",
    "                                            linestyle='--',\n",
    "                                            linewidth=2,\n",
    "                                            zorder=4)\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "        # Prepare labels for the nodes\n",
    "        labels = {}\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            concepts = graph.nodes[node].get('concepts', [])\n",
    "            label = f\"{i + 1}. {concepts[0] if concepts else ''}\"\n",
    "            labels[node] = label\n",
    "\n",
    "        for node in traversal_graph.nodes():\n",
    "            if node not in labels:\n",
    "                concepts = graph.nodes[node].get('concepts', [])\n",
    "                labels[node] = concepts[0] if concepts else ''\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(traversal_graph, pos, labels, font_size=8, font_weight=\"bold\", ax=ax)\n",
    "\n",
    "        # Highlight start and end nodes\n",
    "        start_node = traversal_path[0]\n",
    "        end_node = traversal_path[-1]\n",
    "\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               nodelist=[start_node],\n",
    "                               node_color='lightgreen',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               nodelist=[end_node],\n",
    "                               node_color='lightcoral',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        ax.set_title(\"Graph Traversal Flow\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Add colorbar for edge weights\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Edge Weight', rotation=270, labelpad=15)\n",
    "\n",
    "        # Add legend\n",
    "        regular_line = plt.Line2D([0], [0], color='blue', linewidth=2, label='Regular Edge')\n",
    "        traversal_line = plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Traversal Path')\n",
    "        start_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Start Node')\n",
    "        end_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='End Node')\n",
    "        legend = plt.legend(handles=[regular_line, traversal_line, start_point, end_point], loc='upper left', bbox_to_anchor=(0, 1), ncol=2)\n",
    "        legend.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def print_filtered_content(traversal_path, filtered_content):\n",
    "        \"\"\"\n",
    "        Prints the filtered content of visited nodes in the order of traversal.\n",
    "\n",
    "        Args:\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "        - filtered_content (dict of int: str): A dictionary mapping node indices to their filtered content.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        print(\"\\nFiltered content of visited nodes in order of traversal:\")\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            print(f\"\\nStep {i + 1} - Node {node}:\")\n",
    "            print(f\"Filtered Content: {filtered_content.get(node, 'No filtered content available')[:200]}...\")  # Print first 200 characters\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Graph RAG class\n",
    "\"\"\"\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n",
    "        querying, and visualization.\n",
    "\n",
    "        Attributes:\n",
    "        - llm: An instance of a large language model (LLM) for generating responses.\n",
    "        - embedding_model: An instance of an embedding model for document embeddings.\n",
    "        - document_processor: An instance of the DocumentProcessor class for processing documents.\n",
    "        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n",
    "        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n",
    "        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(temperature=0.2, model=OPENAI_API_MODEL, max_tokens=16384)\n",
    "        self.embedding_model = OpenAIEmbeddings(model=OPENAI_API_EMBEDDING_MODEL)\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.knowledge_graph = KnowledgeGraph(tpm=25000)\n",
    "        self.query_engine = None\n",
    "        self.visualizer = Visualizer()\n",
    "\n",
    "    async def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n",
    "\n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        splits, vector_store = self.document_processor.process_documents(documents)\n",
    "        await self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
    "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n",
    "\n",
    "    async def query(self, query: str):\n",
    "        \"\"\"\n",
    "        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n",
    "\n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "\n",
    "        Returns:\n",
    "        - str: The response to the query.\n",
    "        \"\"\"\n",
    "        response, traversal_path, filtered_content = await self.query_engine.query(query)\n",
    "\n",
    "        if traversal_path:\n",
    "            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n",
    "        else:\n",
    "            print(\"No traversal path to visualize.\")\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings length: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting concepts (batch 1): 100%|██████████| 10/10 [00:01<00:00,  5.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting concepts (batch 2): 100%|██████████| 10/10 [00:02<00:00,  4.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting concepts (batch 3): 100%|██████████| 10/10 [00:02<00:00,  4.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting concepts (batch 4): 100%|██████████| 10/10 [00:01<00:00,  6.13it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Extracting concepts (batch 5): 100%|██████████| 6/6 [00:01<00:00,  4.07it/s]\n",
      "\n",
      "Adding edges: 100%|██████████| 46/46 [00:00<00:00, 35656.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the RAG instance\n",
    "\"\"\"\n",
    "graph_rag = GraphRAG()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Process the document and create the graph\n",
    "\"\"\"\n",
    "await graph_rag.process_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_rag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you update me on any new leads from the past week?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mgraph_rag\u001b[49m\u001b[38;5;241m.\u001b[39mquery(query1)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_rag' is not defined"
     ]
    }
   ],
   "source": [
    "query1 = \"Drop your query here\"\n",
    "response1 = await graph_rag.query(query1)\n",
    "print(response1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sofardatasai-rag-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
